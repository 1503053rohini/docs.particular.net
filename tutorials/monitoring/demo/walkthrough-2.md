---
title: "Monitoring NServiceBus solutions: Demo - System performance"
reviewed: 2017-11-03
summary: How to mesaure inter-endpoint performance and look for congestion with the queue length and critical time metrics.
---

_Which endpoints have the most work to do?_

Each endpoint has a backlog of messages to process in it's input queue. As each endpoint processes it's own backlog it is also generating new messages that go into the backlog of other endpoints in the system. When a spike in traffic occurs, either due to increased activity by end users or because a large batch process kicks off, it can be useful to monitor the changes to backlog load across the system. Using this information you can decide to scale some of your endpoints out to meet the increased demand.

This part of the tutorial guides you finding the endpoints with the greatest backlog of work to do.

include: walkthrough-solution


## Metrics

To measure endpoint backlogs we're going to use two metrics: queue length and critical time.

Queue lenth is an estimate of how many messages are in the input queue of an endpoint, representing the backlog of messages to be processed. If an endpoint is processing messages faster than it is receiving them then queue length will go down. Conversely, when an endpoint is receiving messages faster than it can process them, queue length will go.

Critical time is the time between when a message is initially sent and when it has been completely processed. Critical time includes the time it takes for a message to get from a sending endpoint to the destination queue, the time the message spends waiting to get to the front of that queue to be processed, and the time it takes the receiving endpoint to process the message. Critical time is the time it takes for your system to react to a specific message.

Queue length and critical time are related. As queue length increases, messages spend more time waiting in an endpoint's input queue, and so critical time also increases.

DANGER: Both critical time and queue length are approximations and are not exact measurements.

Criticial time is calculated using timestamps generated by two different machines (sender and receiver). If there is a significant difference in clocks on these machines then it will introduce an error into the critical time calculation. 

Queue length is approximated by have sending and receiving endpoints report how many messages they have sent or received respectively. If reports are received out of order or if there is a significant delay in either of these reports arriving, then it can introduce an error into the queue length calculation.


### Sample walkthrough

The following walk through shows you how to identify changes in endpoint backlog and use that information to scale out endpoints to handle load.

**Run the sample solution. Open ServicePulse to the Monitoring tab.**

![Service Pulse monitoring tab showing sample endpoints](servicepulse-monitoring_tab-sample_low_throughput.png)

Look at the queue length and critical times for each endpoint. By default, the ClientUI endpoint is sending one order per second into the system and the system is able to keep up. Queue lengths are hovering around 0 as messages are not waiting around in input queues. Critical times are remaining very near processing time as most of a messages critical time is the amount of time it takes to process. 

Let's see what happens when we apply more pressure to the system.

**Find the ClientUI endpoint window and toggle High-Throughput mode. Now go back to the ServicePulse Monitoring Tab.**

![Service Pulse monitoring tab showing sample endpoints in high throuhgput mode](servicepulse-monitoring_tab-sample_high_throughput.png)

Now look at the queue length and critical time for each endpoint. Notice that there isn't that much change for the Billing and Shipping endpoints. The Sales endpoint is quickly running in to difficulties though. Once it hits it's throughput limit, the Sales queue length starts to ramp up. As it does, the critical time also starts to climb. The Sales endpoint is becoming less responsive as it's backlog of work increases.

**Once there are 1000 messages in the backlog, turn off High-Throughput mode.**

Now that the spike in traffic is over, there is still a large backlog of messages to get through. At a throughput of 4 messages per second, it's going to take 250 seconds (just over 4 minutes) to get through all of the backlog. That's assuming there are no new messages coming in to the Sales input queue (which isn't true).

To handle this load of traffic we are going to scale out the Sales endpoint.

**In the Monitoring folder, run `scale.bat`.**

This batch file will start 3 more instances of the Sales endpoint (for a total of 4) with a 20 second delay in between starting each instance. As each new instance comes online, the critical time comes down and the throughput increases.

As the throughput of the Sales endpoint increases watch it's effect the Billing and Shipping endpoints. As these endpoints are waiting for `OrderPlaced` events from the Sales endpoint, increasing the throughput of Sales also increases the throughput of these downstream endpoints. Neither of them seems to be building up a large queue length or critical time, so we don't need to scale them out.

NOTE: This scale-out example is contrived and it works because the message handlers are using little/no system resources. In a production scenario you'd likely run a second instance of the endpoint on a different machine.

Once you have more than one instance of an endpoint running, ServicePulse will show a count of how many instances are running in the overview screen.

**In the ServicePulse monitoring tab, click the Sales endpoint to open a detailed view.**

In this view you can see all of the instances that are running.

SCREENSHOT - List of instances that are running

Each instance provides it's own breakdown of metric data. If one particular instance is having a problem you'll be able to see it here. 

**Find the window for instance-2 of the Sales endpoint. Close it.**

When an endpoint instance stops sending data, it will be displayed with an error icon in ServicePulse.










TODO: Review the below. Do we still need to cover this here?


WARNING: Critical time is a delayed measurement. It measures the amount of time a message _took_ to get processed after it was sent. When queue length, network latency, and processing time are relatively stable, then critical time can be used to predict how long a new message will take to get processed. If any of those factors are changing significantly then critical time is less useful as a predictive measurement.


**In the Billing endpoint, enable network latency simulation.**

Under normal operation, when the Billing endpoint recieves an `OrderPlaced` event it publishes a corresponding `OrderBilled` event which gets delivered to the Shipping endpoint. This simulation shows what happens when there is significant network latency between endpoints by delaying the time between when `OrderBilled` is first published and when it appears in the input queue of the Shipping endpoint. 

SCREENSHOT - ServicePulse Monitoring tab - Network Latency Simulation

Look at the Shipping endpoint in the ServicePulse monitoring tab. You can see that it's processing time and queue length remain stable as the endpoint is able to keep up with it's incoming load of messages. The throughput on Shipping might be going down as it is receiving fewer messages over time. Critical time is going up. This is an indication that there is network latency happening somewhere.

**In ServicePulse, click the Shipping endpoint to open the detailed view.**

Look at the Message Types breakdown. Here you can see that the critical time is increasing for the `OrderBilled` event but not for `OrderPlaced` events. This helps to narrow the source of the problem. 

**Restart the sample to reset all values to their defaults. In the Sales endpoint slow down message processing to 2 seconds. Open the ServicePulse main Monitoring tab.**

Look at the Sales endpoint in the ServicePulse monitoring tab. The queue length remains steady as the endpoint is able to keep up with it's load. The critical time for the Sales endpoint has increased to remain in step with the processing time. When queue length is short and network latency is low, processing time will dominate the critical time measurement.

[Next lesson: Scheduled retries](./walkthrough-3.md)